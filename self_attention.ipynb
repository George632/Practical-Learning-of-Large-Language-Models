{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.简化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_weight is tensor([[[0.5508, 0.4492],\n",
      "         [0.5309, 0.4691]],\n",
      "\n",
      "        [[0.5566, 0.4434],\n",
      "         [0.5220, 0.4780]],\n",
      "\n",
      "        [[0.5153, 0.4847],\n",
      "         [0.5186, 0.4814]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0848, -0.2373,  0.7479],\n",
       "         [-0.0890, -0.2324,  0.7399]],\n",
       "\n",
       "        [[-0.1690, -0.2583,  0.7524],\n",
       "         [-0.1685, -0.2413,  0.7298]],\n",
       "\n",
       "        [[-0.1749, -0.2377,  0.6996],\n",
       "         [-0.1733, -0.2369,  0.6989]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 728) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #初始化三个线性映射层\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        #X shape is : [batch_size, seq_len, hidden_dim]\n",
    "        q = self.query(X)\n",
    "        k = self.key(X)\n",
    "        v = self.value(X)\n",
    "\n",
    "        att_weight = F.softmax(q @ k.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim=-1)\n",
    "\n",
    "        print(f'att_weight is {att_weight}')\n",
    "        res = att_weight @ v\n",
    "\n",
    "        return res \n",
    "\n",
    "att_layer = SelfAttentionV1(3)\n",
    "att_layer(torch.rand(3,2,3))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.效率优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5157, 0.4843],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.4877, 0.5123],\n",
      "         [0.4911, 0.5089]],\n",
      "\n",
      "        [[0.4657, 0.5343],\n",
      "         [0.4710, 0.5290]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8945,  0.4268],\n",
       "         [-0.8974,  0.4279]],\n",
       "\n",
       "        [[-0.6319,  0.0989],\n",
       "         [-0.6314,  0.0993]],\n",
       "\n",
       "        [[-0.5164,  0.0375],\n",
       "         [-0.5142,  0.0361]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim, 3 * hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "\n",
    "        weight = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.dim), dim=-1)\n",
    "        output = weight @ V\n",
    "        print(weight)\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3,2,2)\n",
    "\n",
    "self_att = SelfAttentionV2(2)\n",
    "self_att(X)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.加入一些细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.dropout的位置\n",
    "# 2.attention_mask\n",
    "# 3.output 矩阵映射\n",
    "\n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "        self.proj = nn.Linear(hidden_dim, 3 * hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, X, att_mask=None):\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, -1)\n",
    "\n",
    "        att_weight = att_weight = Q @ K.transpose(-1,-2) / math.sqrt(self.dim)\n",
    "        if att_mask != None:\n",
    "            \n",
    "            att_weight.masked_fill(\n",
    "                att_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        att_weight = self.dropout(att_weight)\n",
    "        output = att_weight @ V\n",
    "\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "net = SelfAttentionV3(2)\n",
    "net(X, mask).shape\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.面试写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_weight is tensor([[[0.2955, 0.3588, 0.3457, 0.0000],\n",
      "         [0.2950, 0.3591, 0.3459, 0.0000],\n",
      "         [0.3020, 0.3550, 0.3430, 0.0000],\n",
      "         [0.2946, 0.3593, 0.3461, 0.0000]],\n",
      "\n",
      "        [[0.4963, 0.5037, 0.0000, 0.0000],\n",
      "         [0.5144, 0.4856, 0.0000, 0.0000],\n",
      "         [0.5009, 0.4991, 0.0000, 0.0000],\n",
      "         [0.5005, 0.4995, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0780, 0.0666],\n",
       "         [0.1386, 0.1928],\n",
       "         [0.3819, 0.4381],\n",
       "         [0.3831, 0.4403]],\n",
       "\n",
       "        [[0.5325, 0.5278],\n",
       "         [0.5404, 0.5322],\n",
       "         [0.5345, 0.5289],\n",
       "         [0.5343, 0.5288]],\n",
       "\n",
       "        [[0.5542, 0.5316],\n",
       "         [0.5542, 0.5316],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.5542, 0.5316]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionV4(nn.Module):\n",
    "    def __init__(self, dim:int = 2, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.query =nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "\n",
    "        q = self.query(X)\n",
    "        k = self.key(X)\n",
    "        v = self.value(X)\n",
    "\n",
    "        att_weight = q @ k.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if mask is not None:\n",
    "            att_weight = att_weight.masked_fill(\n",
    "                mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        att_weight = torch.softmax(\n",
    "            att_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "        print(f'att_weight is {att_weight}')\n",
    "\n",
    "        att_weight = self.dropout(att_weight)\n",
    "        output = att_weight @ v\n",
    "\n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "net = SelfAttentionV4(2)\n",
    "net(X, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
